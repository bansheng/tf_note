## 1. 自动编码器
自动编码器是一种神经网络，把它的输入转换为输出，主要有两个部分组成，包括编码器和解码器。
分类
1. Undercomplete Autoencoders缺失自动编码器   编码维度小于输入维度，
2. Regularized Autoencoders正规自动编码器
添加损失函数防止过拟合。
    + Sparse Autoencoders 稀疏自动编码器
    一种自动编码器，除了重建误差之外，还具有训练损失中的稀疏性惩罚。它们通常用于其他任务的分类，例如分类。
    + Denoising Autoencoders (DAE) 去噪自动编码器  
    DAE的输入是应该重建的实际输入的损坏副本。 因此，DAE必须撤消损坏（噪声）以及重建。
    + Contractive Autoencoders (CAE) 压缩自动编码器
    这些类型的自动编码器背后的主要思想是学习数据的表示，该表示对于输入中的微小变化是鲁棒的。
3. Variational Autoencoders 变分自动编码器  
它们最大化训练数据的概率，而不是将输入复制到输出，因此不需要正则化来捕获有用信息。

## 2. 编码器与解码器
![编码器和解码器](https://morvanzhou.github.io/static/results/ML-intro/auto3.png)
### 2.1 图片左边的部分为编码器encoder
    原来有时神经网络要接受大量的输入信息, 比如输入信息是高清图片时, 输入信息量可能达到上千万, 让神经网络直接从上千万个信息源中学习是一件很吃力的工作. 所以, 何不压缩一下, 提取出原图片中的最具代表性的信息, 缩减输入信息量, 再把缩减过后的信息放进神经网络学习. 这样学习起来就简单轻松了. 所以, 自编码就能在这时发挥作用. 通过将原数据白色的X 压缩, 解压 成黑色的X, 然后通过对比黑白 X ,求出预测误差, 进行反向传递, 逐步提升自编码的准确性. 
    训练好的自编码中间这一部分就是能总结原数据的精髓. 可以看出, 从头到尾, 我们只用到了输入数据 X, 并没有用到 X 对应的数据标签, 所以也可以说自编码是一种非监督学习. 到了真正使用自编码的时候. 通常只会用到自编码前半部分.
### 2.2 图片右边的部分为解码器decoder
    至于解码器 Decoder, 我们也能那它来做点事情. 我们知道, 解码器在训练的时候是要将精髓信息解压成原始信息, 那么这就提供了一个解压器的作用, 甚至我们可以认为是一个生成器 (类似于GAN). 那做这件事的一种特殊自编码叫做 variational autoencoders, 你能在这里找到他的具体说明.
