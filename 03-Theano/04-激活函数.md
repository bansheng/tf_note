# 激活函数

## 1. 什么是激活函数

总而言之，激活函数的作用就是使重要的信息被激励，不重要或者反向的信息不被激励。传进来的值，经过这种变化，得到另一种结果，而这正是我们需要的。

## 2. 几种常用的激活函数

Theano 中可以用的激励函数可以在这个[链接](http://deeplearning.net/software/theano/library/tensor/nnet/nnet.html) 中找到。 进入这个链接，以 theano.tensor.nnet.nnet.sigmoid(x) 为例。 Sigmoid 函数就是可以做到，当输入值小于 0 并且越小时，输出值就会越接近 0， 当输入值大于 0 并且越大时，输出值就会越接近 1。常被用于分类问题中。 还有其他几种拓展函数，例如 softplus()，当输入值小于 0 时，输出值接近 0， 当输入值大于 0 时，输出值是非线性的上升关系。可以很好地处理具有非线性的逻辑关系的输入值。 relu()， 当输入值小于 0 时，输出值全部等于 0， 当输入值大于 0 时，输出值是线性关系。 常用的有 softplus()，softmax()，relu() 这三种，当然还有 tanh() 等等。 在实际中可以尝试在不同的神经层中，放入不同的激活函数，尝试得到不同的效果。具体问题具体分析，会发现有些激活函数并不适合当前的问题。

## 3. 应用场景

在隐藏层中，可以用 relu, tanh, softplus 等非线性的激活函数。 在分类问题中，可以用 sigmoid ，softmax 来求概率。例如选择 N 类中概率最大的那一类作为预测值。 在回归问题中，可以什么激活函数都不用，只用 linear function，或者对于 价格，高度 这种非负的变量，可以在输出层用 relu 这种非负的激活函数，使输出值非负。