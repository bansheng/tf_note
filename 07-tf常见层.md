### 1. 卷积函数
#### 1.1 tf.contrib.layers.conv2d
```
tf.contrib.layers.conv2d(
    inputs,
    num_outputs,
    kernel_size,
    stride=1,
    padding='SAME',
    data_format=None,
    rate=1,
    activation_fn=tf.nn.relu,
    normalizer_fn=None,
    normalizer_params=None,
    weights_initializer=initializers.xavier_initializer(),
    weights_regularizer=None,
    biases_initializer=tf.zeros_initializer(),
    biases_regularizer=None,
    reuse=None,
    variables_collections=None,
    outputs_collections=None,
    trainable=True,
    scope=None
)
```
+ inputs: 形状为[batch_size, height, width, channels]的输入。
+ num_outputs：代表输出几个channel
**代表w_i 和 b_i的个数**
+ kernel_size：卷积核大小，不需要带上batch和channel，只需要输入尺寸即可。[5,5]就代表5x5的卷积核，如果长和宽都一样，也可以只写一个数5.  
+ stride：步长，默认是长宽都相等的步长。卷积时，一般都用1，所以默认值也是1.如果长和宽都不相等，也可以用一个数组[1,2]。
+ padding：填充方式，'SAME'或者'VALID'。 SAME卷积计算不足就填充，VALID则选择舍弃。
+ activation_fn：激活函数。默认是ReLU。也可以设置为None
+ weights_initializer：权重的初始化，默认为initializers.xavier_initializer()函数。
+ weights_regularizer：权重正则化项，可以加入正则函数。
+ biases_initializer：偏置的初始化，默认为init_ops.zeros_initializer()函数。
+ biases_regularizer：偏置正则化项，可以加入正则函数。
+ **trainable：是否可训练，如作为训练节点，必须设置为True，默认即可。如果我们是微调网络，有时候需要冻结某一层的参数，则设置为False。**

#### 1.1 tf.contrib.layers.conv2d_transpose (解卷积/反卷积)
```
tf.contrib.layers.conv2d_transpose(
    inputs,
    num_outputs,
    kernel_size,
    stride=1,
    padding='SAME',
    data_format=DATA_FORMAT_NHWC,
    activation_fn=tf.nn.relu,
    normalizer_fn=None,
    normalizer_params=None,
    weights_initializer=initializers.xavier_initializer(),
    weights_regularizer=None,
    biases_initializer=tf.zeros_initializer(),
    biases_regularizer=None,
    reuse=None,
    variables_collections=None,
    outputs_collections=None,
    trainable=True,
    scope=None
)
```
参数基本和上面的conv2d一致 不过这个是做反卷积操作
将输入的input反卷积，转换为指定的size（和stride，kernel_size，num_outputs有关有关

### 2. 池化层
#### 2.1 tf.contrib.layers.max_pool2d
```
tf.contrib.layers.max_pool2d(
    inputs,
    kernel_size,
    stride=2,
    padding='VALID',
    data_format=DATA_FORMAT_NHWC,
    outputs_collections=None,
    scope=None
)
```
+ imputs: [batch_size, height, width, channels]的输入
+ kernel_size：卷积核大小

#### 2.2 tf.contrib.layers.avg_pool2d
```
tf.contrib.layers.avg_pool2d(
    inputs,
    kernel_size,
    stride=2,
    padding='VALID',
    data_format=DATA_FORMAT_NHWC,
    outputs_collections=None,
    scope=None
)
```
+ imputs: [batch_size, height, width, channels]的输入
+ kernel_size：卷积核大小
### 3. 激活层tf.nn.relu

-----
人们将以上三剑客的组合视为特征提取的过程，下面的全连接层作为分类的过程

### 4. 全连接层tf.contrib.layers.fully_connected
```
tf.contrib.layers.fully_connected(
    inputs,
    num_outputs,
    activation_fn=tf.nn.relu,
    normalizer_fn=None,
    normalizer_params=None,
    weights_initializer=initializers.xavier_initializer(),
    weights_regularizer=None,
    biases_initializer=tf.zeros_initializer(),
    biases_regularizer=None,
    reuse=None,
    variables_collections=None,
    outputs_collections=None,
    trainable=True,
    scope=None
)
```
+ inputs: A tensor of at least rank 2 and static value for the last dimension; i.e. [batch_size, depth], [None, None, None, channels]. 至少两项，是一个二维矩阵。
+ num_outputs: Integer or long, the number of output units in the layer.分类的数目
将所有特征集合，对结果进行分类，分类的数目指定

### 5. 随机丢失层tf.contrib.layers.dropout
```
tf.contrib.layers.dropout(
    inputs,
    keep_prob=0.5,
    noise_shape=None,
    is_training=True,
    outputs_collections=None,
    scope=None,
    seed=None
)
```
inputs: Tensor input.
rate: The dropout rate, between 0 and 1. E.g. "rate=0.1" would drop out 10% of input units.

**防止过拟合**
就是你在训练的时候想拿掉多少神经元，按比例计算。1就是没有dropout，0就是整个层都没了（会报错的）。但是其他的数值会乘以1/keep_prob,期待总和不变
```
import tensorflow as tf

x1 = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=tf.float32)
y3 = tf.contrib.layers.dropout(x1, keep_prob=0.2)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(y3))
>>>
[  0.   0.   0.   0.  25.   0.  35.   0.   0.   0.]
```

### 6. 嵌入图层
combines all the channels except the first one indicating the batches. So the dimension of [batch_size, width, height, channel] becomes [batch_size, width x height x channel].  
将所有的通道结合起来，矩阵维度下降为2维

This is the last fully-connected layer prior to softmax which the number of its output units must be equal to the number of classes. The output of this layer has the dimensionality of [batch_size, 1, 1, num_classes].   
这是softmax之前的最后一个完全连接的层，其输出单元的数量必须等于类的数量。