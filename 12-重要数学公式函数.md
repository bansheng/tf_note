### 1. sigmod函数
![sigmod函数](https://upload.wikimedia.org/wikipedia/commons/3/33/Sigmoid_function_01.png)
```math
    f(x) = 1 / 1 + e^{x}
```

优点
+ 他的输入范围是−∞→+∞ ，而之于刚好为（0，1），正好满足概率分布为（0，1）的要求。我们用概率去描述分类器，自然比单纯的某个阈值要方便很多； 
+ 2.他是一个单调上升的函数，具有良好的连续性，不存在不连续点。
缺点
+ 梯度饱和在取值0或者1的时候
+ 输入为0，输出不为0

### 2.  tanh激活函数
tanh函数将一个实数输入映射到[-1,1]范围内。当输入为0时，tanh函数输出为0。
![tanh函数](https://upload.wikimedia.org/wikipedia/commons/thumb/7/76/Sinh_cosh_tanh.svg/600px-Sinh_cosh_tanh.svg.png)
```math
    f(x) = (e^x - e^{-x}) / (e^x + e^{-x})
```

### 3. ReLU（The Rectified Linear Unit激活函数
![RelU函数](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c9/Ramp_function.svg/650px-Ramp_function.svg.png)
```math
f(x) = max(0, x)
```

### 4. 常见损失函数
机器学习或者统计机器学习常见的损失函数如下：

1. 0-1损失函数 （0-1 loss function）
```math
L(Y,f(X))= 1,  Y = f(X)

L(Y,f(X))= 0,  Y ≠ f(X)
```

2. 平方损失函数（quadratic loss function) 
```math
L(Y,f(X))=(Y−f(x)) ^2
```

3. 绝对值损失函数(absolute loss function) 
```math
L(Y,f(x))=|Y−f(X)|
```

4. 对数损失函数（logarithmic loss function) 或对数似然损失函数(log-likehood loss function)  
```math
L(Y,P(Y|X))=−logP(Y|X)
```
逻辑回归中，采用的则是对数损失函数。如果损失函数越小，表示模型越好。

### 5. 