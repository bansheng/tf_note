## 1. sigmod函数
![sigmod函数](https://upload.wikimedia.org/wikipedia/commons/3/33/Sigmoid_function_01.png)
```math
    f(x) = 1 / 1 + e^{x}
```

优点
+ 他的输入范围是−∞→+∞ ，而之于刚好为（0，1），正好满足概率分布为（0，1）的要求。我们用概率去描述分类器，自然比单纯的某个阈值要方便很多； 
+ 2.他是一个单调上升的函数，具有良好的连续性，不存在不连续点。
缺点
+ 梯度饱和在取值0或者1的时候
+ 输入为0，输出不为0

## 2.  tanh激活函数
tanh函数将一个实数输入映射到[-1,1]范围内。当输入为0时，tanh函数输出为0。
![tanh函数](https://upload.wikimedia.org/wikipedia/commons/thumb/7/76/Sinh_cosh_tanh.svg/600px-Sinh_cosh_tanh.svg.png)
```math
    f(x) = (e^x - e^{-x}) / (e^x + e^{-x})
```

## 3. ReLU（The Rectified Linear Unit激活函数
![RelU函数](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c9/Ramp_function.svg/650px-Ramp_function.svg.png)
```math
f(x) = max(0, x)
```

## 4. 常见损失函数
机器学习或者统计机器学习常见的损失函数如下：

1. 0-1损失函数 （0-1 loss function）
```math
L(Y,f(X))= 1,  Y = f(X)

L(Y,f(X))= 0,  Y ≠ f(X)
```

2. 平方损失函数（quadratic loss function) 
```math
L(Y,f(X))=(Y−f(x)) ^2
```

3. 绝对值损失函数(absolute loss function) 
```math
L(Y,f(x))=|Y−f(X)|
```

4. 对数损失函数（logarithmic loss function) 或对数似然损失函数(log-likehood loss function)  
```math
L(Y,P(Y|X))=−logP(Y|X)
```
逻辑回归中，采用的则是对数损失函数。如果损失函数越小，表示模型越好。

## 5. 高级激活层Advanced Activation
### 5.1 LeakyReLU层
```
keras.layers.advanced_activations.LeakyReLU(alpha=0.3)
```
LeakyRelU是修正线性单元（Rectified Linear Unit，ReLU）的特殊版本，当不激活时，LeakyReLU仍然会有非零输出值，从而获得一个小梯度，避免ReLU可能出现的神经元“死亡”现象。即
```
f(x)=alpha * x for x < 0, f(x) = x for x>=0
```
参数
+ alpha：大于0的浮点数，代表激活函数图像中第三象限线段的斜率
+ 输入shape：任意，当使用该层为模型首层时需指定input_shape参数
+ 输出shape：与输入相同
>参考文献
>Rectifier Nonlinearities Improve Neural Network Acoustic Models
### 5.2 PReLU层
```
keras.layers.advanced_activations.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)
```
该层为参数化的ReLU（Parametric ReLU），表达式是：
```math
f(x) = alpha * x for x < 0, f(x) = x for x>=0
```
**此处的alpha为一个与xshape相同的可学习的参数向量。**

参数
+ alpha_initializer：alpha的初始化函数
+ alpha_regularizer：alpha的正则项
+ alpha_constraint：alpha的约束项
+ shared_axes：该参数指定的轴将共享同一组科学系参数，例如假如输入特征图是从2D卷积过来的，具有形如(batch, height, width, channels)这样的shape，则或许你会希望在空域共享参数，这样每个filter就只有一组参数，设定shared_axes=[1,2]可完成该目标
+ 输入shape 任意，当使用该层为模型首层时需指定input_shape参数
+ 输出shape 与输入相同

>参考文献
> Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification
### 5.3 ELU层
```
keras.layers.advanced_activations.ELU(alpha=1.0)
```
ELU层是指数线性单元（Exponential Linera Unit），表达式为： 该层为参数化的ReLU（Parametric ReLU），表达式是：
```
f(x) = alpha * (exp(x) - 1.) for x < 0, f(x) = x for x>=0
```
参数
+ alpha：控制负因子的参数
+ 输入shape 任意，当使用该层为模型首层时需指定input_shape参数
+ 输出shape 与输入相同

参考文献
>Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)

### 5.4 ThresholdedReLU层

```python
keras.layers.advanced_activations.ThresholdedReLU(theta=1.0)
```

该层是带有门限的ReLU，表达式是：

```math
f(x) = x for x > theta,f(x) = 0 otherwise
```

参数

+ theata：大或等于0的浮点数，激活门限位置
+ 输入shape 任意，当使用该层为模型首层时需指定input_shape参数
+ 输出shape 与输入相同
