## 1. tf.exp
```
tf.math.exp(
    x,
    name=None
)
```
计算
```math
e^x
```
返回的也是一个Tensor

## 2. tf.reduce_sum
降维求和
```
import tensorflow as tf

x = tf.constant([[1, 1, 1], [1, 1, 1]])
tf.reduce_sum(x, 0)  
# [2, 2, 2] 
tf.reduce_sum(x, 1)  
# [3, 3] 
tf.reduce_sum(x, [0, 1])  
# 6 
```

## 4. tf.nn.softmax
计算softmax激活
```
tf.nn.softmax(
    logits,
    axis=None,
    name=None,
    dim=None
)
```
    softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)


## 5. tf.reduce_mean
```
tf.math.reduce_mean(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
```
计算在axis维上的平均值，输出维度会减少1，如果axis为None，只输出一个值。  
如果keepdims为True，保持维度，长度为1.
```
x = tf.constant([[1., 1.], [2., 2.]])
tf.reduce_mean(x)  # 1.5
tf.reduce_mean(x, 0)  # [1.5, 1.5]
tf.reduce_mean(x, 1)  # [1.,  2.]
```


## 6. tf.clip_by_value
```
tf.clip_by_value(
    t,
    clip_value_min,
    clip_value_max,
    name=None
)
```
将t的值修改在min到max之间，如果小于min改为min，大于max改为max，在这之间则不修改

## 7. tf.squared_difference
返回差的平方
```
tf.math.squared_difference(
    x,
    y,
    name=None
)
```
返回`(x-y)(x-y)`

## 8. tf.argmax
```
tf.math.argmax(
    input,
    axis=None,
    name=None,
    dimension=None,
    output_type=tf.dtypes.int64
)
```
返回指定维度最大值的下标
```
a = tf.Variable([[0, 2, 1], [3, 2, 3]]
tf.argmax(a, 0)
> [1, 0, 1]

tf.argmax(a, 1)
> [1, 0]
```


## 9. tf.equal
```
tf.math.equal(
    x,
    y,
    name=None
)
```
返回(x == y)，注意这可以是一个矩阵
```
a = tf.Variable([[1, 2, 3], [3, 2, 1]], dtype=tf.int32)
b = tf.Variable([[0, 2, 1], [3, 2, 3]], dtype=tf.int32)
c = tf.argmax(a, 1)
d = tf.argmax(b, 1)
e = tf.equal(c, d)
```
    > [False,  True]
    
## 10. tf.cast
```
tf.dtypes.cast(
    x,
    dtype,
    name=None
)
```
类型转换，转换为指定类型
```python
x = tf.constant([1.8, 2.2], dtype=tf.float32)
tf.cast(x, tf.int32)  # [1, 2], dtype=tf.int32
```

## 11. tf.reshape()
```
tf.reshape(
    tensor,
    shape,
    name=None
)
```
将输入的tensor转换成shape为输入shape的tensor输出。-1做推断。
```
# tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]
# tensor 't' has shape [9]
reshape(t, [3, 3]) ==> [[1, 2, 3],
                        [4, 5, 6],
                        [7, 8, 9]]

# tensor 't' is [[[1, 1], [2, 2]],
#                [[3, 3], [4, 4]]]
# tensor 't' has shape [2, 2, 2]
reshape(t, [2, 4]) ==> [[1, 1, 2, 2],
                        [3, 3, 4, 4]]

# tensor 't' is [[[1, 1, 1],
#                 [2, 2, 2]],
#                [[3, 3, 3],
#                 [4, 4, 4]],
#                [[5, 5, 5],
#                 [6, 6, 6]]]
# tensor 't' has shape [3, 2, 3]
# pass '[-1]' to flatten 't'
reshape(t, [-1]) ==> [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]

# -1 can also be used to infer the shape

# -1 is inferred to be 9:
reshape(t, [2, -1]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],
                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]
# -1 is inferred to be 2:
reshape(t, [-1, 9]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],
                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]
# -1 is inferred to be 3:
reshape(t, [ 2, -1, 3]) ==> [[[1, 1, 1],
                              [2, 2, 2],
                              [3, 3, 3]],
                             [[4, 4, 4],
                              [5, 5, 5],
                              [6, 6, 6]]]

# tensor 't' is [7]
# shape `[]` reshapes to a scalar
reshape(t, []) ==> 7
```

## 12. tf.square()
```
tf.math.square(
    x,
    name=None
)
return x*x
```

## 13. tf.one_hot()
```
tf.one_hot(
    indices,
    depth,
    on_value=None,
    off_value=None,
    axis=None,
    dtype=None,
    name=None
)
```
升维操作，将n维变量升为n+1维。
`indice`的大小为a1 * a2 * a3 * ...
+ axis=-1 输出为a1 * a2 * a3 * .. * depth
+ axis=0 输出为depth * a1 * a2 * a3 * ..
+ axis=1 输出为a1 * a2 * a3 * depth * ..
+ ..  

on_value默认为1
off_value默认为0

输出的第depth维第indice位(indice < depth && indice > 0, 否则全部输出off_value)值为on_value，其余位置为off_value。

```
indices = [0, 1, 2]
depth = 3
tf.one_hot(indices, depth)  # output: [3 x 3]
# [[1., 0., 0.],
#  [0., 1., 0.],
#  [0., 0., 1.]]

indices = [0, 2, -1, 1]
depth = 3
tf.one_hot(indices, depth,
           on_value=5.0, off_value=0.0,
           axis=-1)  # output: [4 x 3]
# [[5.0, 0.0, 0.0],  # one_hot(0)
#  [0.0, 0.0, 5.0],  # one_hot(2)
#  [0.0, 0.0, 0.0],  # one_hot(-1)
#  [0.0, 5.0, 0.0]]  # one_hot(1)

indices = [[0, 2], [1, -1]]
depth = 3
tf.one_hot(indices, depth,
           on_value=1.0, off_value=0.0,
           axis=-1)  # output: [2 x 2 x 3]
# [[[1.0, 0.0, 0.0],   # one_hot(0)
#   [0.0, 0.0, 1.0]],  # one_hot(2)
#  [[0.0, 1.0, 0.0],   # one_hot(1)
#   [0.0, 0.0, 0.0]]]  # one_hot(-1)
```

## 14. tf.random.naomal
```
tf.random.normal(
    shape,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
```
返回一个随机正态分布

## 15. tf.squeeze && tf.expand_dim
```
降维函数
tf.squeeze(
    input,
    axis=None,
    name=None,
    squeeze_dims=None
)
升维函数
tf.expand_dims(
    input,
    axis=None,
    name=None,
    dim=None
)
```
### tf.squeeze
去除所有值为1的维度，或者指定值为1的维度。注意能去除的维度值必须为1.
```
# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]
tf.shape(tf.squeeze(t))  # [2, 3]

# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]
tf.shape(tf.squeeze(t, [2, 4]))  # [1, 2, 3, 1]
```
### tf.expand_dim
指定位置进行维度拓展
```
# 't' is a tensor of shape [2]
tf.shape(tf.expand_dims(t, 0))  # [1, 2]
tf.shape(tf.expand_dims(t, 1))  # [2, 1]
tf.shape(tf.expand_dims(t, -1))  # [2, 1]

# 't2' is a tensor of shape [2, 3, 5]
tf.shape(tf.expand_dims(t2, 0))  # [1, 2, 3, 5]
tf.shape(tf.expand_dims(t2, 2))  # [2, 3, 1, 5]
tf.shape(tf.expand_dims(t2, 3))  # [2, 3, 5, 1]
```